
Computer Science 188 - Introduction to Machine Learning

http://web.cs.ucla.edu/~sriram/courses/cs188.winter-2017/html/index.html

==============
Jan. 10, 2017
==============

mini-quiz (30 mins) on Tue. Jan. 17.

Course in machine learning by Hal Daume III (CIML)
Machine Learning: The art and science of algorithms that make sense of data by
Peter Flach (FL)

Exams
------
Midterm (Feb. 14)
Final (Mar. 22)

closed book, closed notes

Python 2.7
-----------
numpy
scipy
scikit-learn



When do we use ML?
-------------------
- human expertise does not exist
  + navigating on Mars
- humans cannot explain their expertise
  + speech recognition
- algorithms must be customized
  + personalized medicine
- data exists to acquire expertise
  + genomics
- recognizing patterns
  + facial identities or facial expressions
  + handwritten or spoken words
  + medical images
- generating patterns
  + generating images or motion sequences
- recognizing anomalies
  + unusual credit card transactions
  + unusual patterns of sensor readings in a nuclear power plant
- prediction
  + future stock prices or currency exchange rates


Machine Learning
-----------------
Study of algorithms that
- improve their performance P
- at some task T
- with experience E
A well-defined learning task is given by <P, T, E>


Defining the Learning Task
---------------------------
Improve on task T with respect to performance metric P, based on experience E

T: Playing checkers
P: Percentage of games won agianst arbitrary opponents
E: Playing practice games agianst itself

T: Recognizing hand-written words
P: Percentage of words correctly classified
E: Database of human-labeled images of handwritten words

T: Driving on four-lane highways using vision sensors
P: Average distance traveled before a human-judged error
E: Sequence of images and steering commands recorded while observing a human
   driver

T: Categorize email messages as spam or legitimate
P: Percentage of email messages correctly classified
E: Database of emails, some with human-given labels



TYPES OF LEARNING


Supervised (Inductive) Learning
--------------------------------
Learning with a teacher
Given: labeled traning instances (examples)
Goal: learn mapping that predicts label for test instance

  Regression
  -----------
  Given (x1, y1), (x2, y2), ... , (xn, yn)
  Learn a function f(x) to predict y given x
  * y is real-valued == regression

  Classification
  ---------------
  Given (x1, y1), (x2, y2), ... , (xn, yn)
  Learn a function f(x) to predict y given x
  * y is categorical == classification

x can be multi-dimensional
each dimension corresponds to an attribute


Unsupervised Learning
----------------------
Learning without a teacher
Given: unlabeled inputs
Goal: learn some intrinsic structure in inputs

Given x1, x2, ... , xn (without labels)
Output hidden structure behind the x's

- clustering
- gemonics application
  group individuals by genetic similarity
- independent component analysis
  separate a combined signal into its original sources
  two people are talking, separate the voices


Reinforcement Learning
-----------------------
Learning by interacting
Given: agent interacting in environment (having set of states)
Goal: learn policy (state to action mapping) that maximizes agent's reward

Given sequence of states and actions with (delayed) rewards
Learn policy that maximizes agent's reward

- game playing
  given sequences of moves and whether or not the player won at the end,
  learn to make good moves
- robot in maze



FRAMING A LEARNING PROBLEM


Representing Instances/Examples
--------------------------------
What is an instance?
How is it represented?

- Define a list of features.
  This is how the algorithm views the data.
  Features are the questions we can ask about the instances.
  
  Red Apple: red, round, leaf, 3oz, ...
  Green Apple: green, round, no leaf, 4oz, ...
  Yellow Banana: yellow, curved, no leaf, 4oz, ...
  Green Banana: green, curved, no leaf, 5oz, ...

During learning/traning/induction, learn a model of what distinguishes apples
and bananas based on the features.
Generate a classifier.

With a new instance, apply the classifier.
Classifier classifies a new instance based on the features.


Learning Algorithm
-------------------
Learning is about *generalizing* from training data
What does this *assume* about training and test set?

- Don't see a test instance exactly the same as a training instance


Generalization Ability: performance of the learning algorithm
How do we measure performance depends on the problem we are trying to solve
The training and test data should be strongly related


Loss function L(y, y^)
  tells us how bad the system's prediction of y is compared to the true
  value of y

  - A loss function for regression (squared loss)
    L(y, y^) = (y - y^)^2

  - A loss function for classification
    L(y, y^) = 0   if y = y^
               1   otherwise

Use the probabilistic model of learning
There is some unknown probability distribution p over instance/label pairs
called the data generating distribution


Learning Problem defined by
  loss function: measures of performance
  data generating distribution: what data do we expect to see
    characterizes experience


Problem Setting
  set of possible instances X
  set of possible labels Y
  unknown target function f: X -> Y
  set of function hypothesis: H = { h | h: X -> Y }
Input
  training instances drawn from the dta generating distribution p
    { (xi, yi) }i = 1 -> n  =  { (x1, y1), ... , (xn, yn) }
Output
  hypothesis h in H that best approximates f
  h should do well (as measured by the loss) on future instances
  formally, h should have low expected (test) loss/risk

    E((x,y)~p)[L(y,h(x))] = sum(x,y) p(x,y)L(y,h(x))

Problem: we don't know what p is
         but we are given samples drawn from p

we instead approximate the risk by the training error/empirical risk
  
  1/n sum(i = i -> n) L(yi, h(xi))

why is this reasonable?
  both the training data and the test data set are generated based on
  this distribution

Problem: can make the training error zero by memorizing


Regression
-----------
For regression, the common choice is squared loss

  L(yi,h(xi)) = (yi - h(xi))^2

Empirical loss of function h applied to training data is then

  1/n sum(i = 1 -> n) L(yi,h(xi)) = 1/n sum(i = 1 -> n) (yi - h(xi))^2


Fundamental Difficulties of Machine Learning
---------------------------------------------
we have access to the training error but really care about the expected loss
our learned funciton needs to generalize beyond the training data

Key Issues in Machine Learning
-------------------------------
Representation:  how to choose a hypothesis space?
  - often we use prior knowledge to guide this choice
  - the ability to answer the next two questions also affects choice
  - which spaces have been useful in practical applications and why?

Optimization:  how to find the best hypothesis within this space?
  - (the ALGORITHMIC question), at the intersection of computer science
    and optimization research
  - are some learning problems computationally intractable?
    (the COMPUTATIONAL quesion)

Evaluation:  how to gauge the accuracy of a hypothesis on unseen testing data?
  - the previous exampe showed that choosing the hypothesis which simply
    minimizes training set error (i.e. empirical loss) is not optimal
  - this question is the main topic of learning theory
  - how can we have confidence in the results?
    (the STATISTICAL question)

Formulation:  how can we formulate application problems as machine learning
              problems? (the ENGINEERING question)

Machine Learning in Practice
-----------------------------
  +
L | understand domain, prior knowledge, and goals
O | data integration, selection, cleaning, pre-processing
O | learn models
P | interpret results
  +
