\documentclass{scrartcl}

\KOMAoptions{
  fontsize=8pt,
}

\addtokomafont{disposition}{\rmfamily}

\usepackage{bm}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{float}
\usepackage{ragged2e}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{geometry}
\geometry{letterpaper, left=.1in, right=.1in, top=.05in, bottom=.07in}
\usepackage{multicol}

\DeclareOldFontCommand{\bf}{\normalfont\bfseries}{\mathbf}

\pagenumbering{gobble}
\setlength{\parskip}{0.2em}
\setlength\parindent{0pt}

\begin{document}
\raggedright
\begin{multicols*}{3}



{\bf Taylor Series}

$\frac{1}{x} = \sum_{n=0}^{\infty}x^n$

$e^x = \sum_{n=0}^{\infty}\frac{x^n}{n!}$

$\cos x = \sum_{n=0}^{\infty}(-1)^n\frac{x^{2n}}{(2n)!}$

$\sin x = \sum_{n=0}^{\infty}(-1)^n\frac{x^{2n+1}}{(2n+1)!}$

$\ln(1+x) = \sum_{n=1}^{\infty}(-1)^{n+1}\frac{x^n}{n}$



{\bf Permutations and Combinations}

$P(n,k)=\frac{n!}{(n-k)!}$

$C(n,k) = {n\choose k} = \frac{n!}{(n-k)!k!}$



{\bf Laplace Transforms}

$F^*(s)=\int_0^\infty f(t)e^{-st}dt$

$f(t) = \int_0^\infty F^*(s)e^{st}ds$



{\bf Convolution Property}

$f(t)*g(t)=\int_0^t f(t-x)g(x)dx \leftrightarrow F^*(s)G^*(s)$



{\bf Z-Transform}

Mapping of discrete function $f_n$ into complex fuction with variable $z$.

$F(z)=\sum_{n=0}^{\infty}f_nz^n$



{\bf Probability and Conditional}

$P(A\cup B) = P(A) + P(B) - P(A\cap B)$

$P(A|B) = \frac{P(A\cap B)}{P(B)}$

$A$, $B$ are independent if $P(A,B)=P(A)P(B)$



{\bf Total Probability}

$P(B) = \sum_{i} P(A_i)P(B|A_i)$



{\bf Bayes' Rule}

$P(A_i|B) = \frac{P(A_i\cap B)}{P(B)} = \frac{P(A_i)P(B|A_i)}{P(B)} = \frac{P(A_i)P(B|A_i)}{\sum_jP(A_j)P(B|A_j)}$



{\bf PMF (Probability Mass Function)}

$p_X(x)=p(\{s\in \Omega\text{ s.t. }X(s)=x\})$

$\sum_xp_X(x)=1$



{\bf Bernoulli Random Variable}

$X=1$ on success, $X=0$ on failure.

$p(X=x) = p$, if $x=1$

$p(X=x) = 1-p$, if $x=0$



{\bf Geometric Random Variable}

Counts \#trials until first success.

$p_X(x)=(1-p)^{x-1}p$, $x=1,2,\cdots$

$p(X\geq s+1|X\geq t)=p(X\geq s)$



{\bf Binomial Random Variable}

Counts \#success in $n$ identical independent experiments.

$p_X(x)={n\choose x}p^x(1-p)^{n-x}$, when $0\leq x\leq n$

$p_X(x)=0$, otherwise



{\bf Poisson Random Variable}

Model occurrence of event over time interval assuming event happens at rate $\lambda$

$p_X(x)=e^{-\lambda}\frac{\lambda^x}{x!}$, when $x=0,1,\cdots$



{\bf PDF (Probability Density Function)}

$\int_{-\infty}^\infty f_X(x)dx=1$



{\bf CDF (Cumulative Distribution Function)}

$F_X(x)=P(X\leq x)$

$\lim_{x\to -\infty}F_X(x)=0$

$\lim_{x\to\infty}F_X(x)=1$

$P(a<X\leq b)=F_X(b)-F_X(a)$



{\bf Uniform Distribution}

$f_X(x)=\frac{1}{b-a}$, when $a\leq x\leq b$

$f_X(x)=0$, otherwise



{\bf Exponential Distribution}

Memoryless continuous distribution.

$f_X(x)=\lambda e^{-\lambda x}$, when $x\geq 0$

$F_X(x)= 1-e^{-\lambda x}$, when $x\geq 0$

$F_X(x)=0$, otherwise

$P(X>x)=e^{-\lambda x}$



{\bf Expectation}

$E[X] = \sum_xxp(x)$

$E[X] = \int_{-\infty}^{\infty}xf_X(x)dx$

If $Y=g(X)$, $E[Y] = \sum_xg(x)p(x)$, $E[Y] = \int_{-\infty}^{\infty}g(x)f_X(x)dx$

$E[X+Y] = E[X] + E[Y]$

$E[aX] = aE[X]$

$E[XY] = E[X]E[Y]$, if $X$,$Y$ are independent.

For $X$, $Y$ with joint PMF $p(x,y)$ or PDF $f_{X,Y}(x,y)$, $E[XY] = \sum_{(x,y)}xyp(x,y)$, $E[XY] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xyf_{X,Y}(x,y)dxdy$



{\bf Conditional Expectation}

$X$,$Y$ are random variables, $E[Y|X] = \sum_yyP(Y=y|X=x)=\sum_yyp_{Y|X}(y|x)$, $E[Y|X] = \int_{-\infty}^{\infty}yf_{Y|X}(y|x)dy$



{\bf Unconditional Expectation}

$E[Y] = \sum_xE[Y|X]p_X(x)$

$E[Y] = \int_{\infty}^{\infty}E[Y|X]f_X(x)dx$



{\bf Variance}

$Var[X] = E[(X-E[X])^2] = \sum_x(x-E[X])^2p(x) = \int_{\infty}^{\infty}(x-E[X])^2f_X(x)dx$

$Var[X]=E[X^2]-E[X]^2$

$Var[X+Y]=Var[X]+Var[Y]+2Cov(X,Y)$

$Var[XY]=E[X^2Y^2]-E^2[XY] = E[X^2]E[Y^2]+Cov(X^2,Y^2)-(E[X]E[Y]+Cov(X,Y))^2$



{\bf Expectations and Variances}

Binomial: $np$, $np(1-p)$

Geometric: $\frac{1}{p}$, $\frac{1-p}{p^2}$

Uniform: $\frac{a+b}{2}$, $\frac{(b-a)^2}{12}$

Exponential: $\frac{1}{\lambda}$, $\frac{1}{\lambda^2}$

Poisson: $\lambda$, $\lambda$



{\bf Covariance}: measure of joint probability

$Cov(X,Y) = E[(X-E[X])(Y-E[Y])]$

$Cov(X,Y) = E[XY] - E[X]E[Y]$

If $X$,$Y$ are independent, $Cov(X,Y)=0$



{\bf Correlation}: scaled version of covariance

$\rho(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$, range $[-1,1]$


{\bf Stationary Process}: $F_X(\bm{x};\bm{t})=F_X(\bm{x};\bm{t}+\tau)$

{\bf Independent Processs}: $F_X(\bm{x};\bm{t})=F_{X_1}(x_1,t_1)F_{X_2}(x_2,t_2)\cdots F_{X_n}(x_n,t_n)$ \\
$f_X(\bm{x},\bm{t})=\prod_{i=1}^nf_{X_i}(x_i,x_t)$ {\bf (Continuous State)} \\
$p_X(\bm{x},\bm{t})=\prod_{i=1}^np_{X_i}(x_i,t_i)$ {\bf (Discrete State)}

{\bf Markovian Property}: $P[X(t_{n+1})\leq x_{n+1}|X(t_n)=x_n,X(t_{n+2})=x_{n+2},\cdots,X(t_0)=x_0]=P[X(t_{n+1})\leq x_{n+1}|X(t_n)=x_n]$


{\bf Discrete Time Markov Chains (DTMC)}: $p_{ij} = P[X_n=j|X_{n-1}=i]$ {\bf (Homogenenous)} \\
$\bm{P}=(p_{ij})=\begin{bmatrix}
p_{00} & p_{01} & \cdots \\
p_{10} & p_{11} & \cdots \\
\vdots & \vdots & \vdots \\
\end{bmatrix}$\\
$\sum_jp_{ij}=1$ for each row.

{\bf Initial State Probabilities}: $\bm{\pi}^{(0)}=(\pi_0^{(0)},\pi_1^{(0)},\cdots)$, where $\pi_j^{(0)}=P[X_0=j]$

{\bf $n$-Step Transition Probabilities}: $p_{ij}^{(n)}=P[X_n=j|X_0=i]=P[X_{n+k}=j|X_k=i]$

{\bf Chapman-Kolmogorov}: $p_{ij}^{(n)}=\sum_kp_{ik}^{(n-1)}p_{kj}$

{\bf Limiting Distribution}: $\pi=\lim_{n\to\infty}\pi^{(0)}P^n$ \\
$\pi=\pi\bm{P}$ and $\sum_j\pi_j=1$



{\bf Continuous Time Markov Chains (CTMC)}: State transitions permitted at arbitrary time instances. Time spent in a state is exponentially distributed.

{\bf State Transition Probability}: $p_{ij}(t) = p(X(\tau+t)=j|X(\tau)=i)$

{\bf Chapman-Kolmogorov Equation}: $p_{ij}(s+t) = \sum_kp_{ik}(s)p_{kj}(t)$

{\bf Transition Probability}:
$\bm{H}(t) = \{p_{ij}(t)\}$ \\
$\bm{H}(s+t) = \bm{H}(s)\bm{H}(t)$ \\
$\bm{H}(t+\Delta t) = \bm{H}(t)\bm{H}(\Delta t)$ \\
$\bm{H}(t+\Delta t) = \bm{H}(t)[\bm{H}(\Delta t)-I]$ \\
$\frac{d\bm{H}(t)}{dt} = \bm{H}(t)\lim_{\Delta t\to 0}[\frac{\bm{H}(\Delta t) - I}{\Delta t}]$ \\
$\bm{Q} = \lim_{\Delta t\to 0}[\frac{\bm{H}(\Delta t)-I}{\Delta t}]$ \\
$\frac{d\bm{H}(t)}{dt} = \bm{H}(t)\bm{Q}$

{\bf Transition Rate Matrix}: $\bm{Q}$, infinitesimal generator of $\bm{H}(t)$. $\bm{H}(t)=e^{\bm{Q}t}$.

{\bf Diagonal Elements} $\leq 0$. $q_{ii}=\lim_{\Delta t\to 0}[\frac{p_{ii}(\Delta t)-1}{\Delta t}]$.

{\bf Off-Diagonal Elements} $\geq 0$. $q_{ij}=\lim_{\Delta t\to 0}[\frac{p_{ij}(\Delta t)-0}{\Delta t}]$, for $i\neq j$.

In each row, sum of off-diagonal = magnitude of diagonal: $q_{ii}=-\sum_{i\neq j}q_{ij}$

{\bf State Probabilities}:\\ $\pi(t)=\pi(0)H(t)$, $\;\pi(t)=\pi(0)e^{Qt}$

{\bf Stationary Distribution}: $\pi Q=0$, $\;\sum_j\pi_j=1$


{\bf Birth-Death Process}: At state $k$, $\lambda_k$, $\mu_k$ are birth and death rates. {\bf Transition Matrix}: $Q=\begin{bmatrix}
-\lambda_0 & \lambda_0 & 0 & 0 & \cdots & \cdots \\
\mu_1 & -(\lambda_1+\mu_1) & \lambda_1 & 0 & \cdots & \cdots \\
0 & \mu_2 & -(\lambda_2+\mu_2) & \lambda_2 & 0 & \cdots \\
\cdots & \cdots & \cdots & \cdots & \cdots & \cdots
\end{bmatrix}$

{\bf Equilibrium Solution}: $\pi\bm{Q}=0$ and $\sum_j\pi_j=1$

{\bf Differential Difference Equations}: $\frac{\partial\pi_k(t)}{\partial t} = \lambda_{k-1}\pi_{k-1}(t)+\mu_{k+1}\pi_{k+1}(t)-(\lambda_k+\mu_k)\pi_k(t)$ \\
$\frac{\partial\pi_0(t)}{\partial t} = \mu_1\pi_1(t) - \lambda_0\pi_0(t)$ \\
$\frac{\partial\pi_k(t)}{\partial t} = $ flow in - flow out \\
flow in $=\lambda_{k-1}\pi_{k-1}+\mu_{k+1}\pi_{k+1}$ \\
flow out $= (\lambda_k+\mu_k)\pi_k$


{\bf Queueing Theory}: Analyze different systems. Poisson distribution model arrival process, exponential distribution model service times.

{\bf Poisson Process}: {\bf ArrivalRate of $\lambda$}: $p(n\text{ arrivals in interval } T)=\frac{(\lambda T)^ne^{-\lambda T}}{n!}$\\
{\bf Exponential Inter-Arrival Time}: $p(\text{inter-arrival}\leq T) = 1-e^{-\lambda T}$

{\bf Merging Property}: Let $A_1$, $A_2$, ..., $A_k$ be independent Poisson processes with rates $\lambda_1$, $\lambda_2$, ..., $\lambda_k$, then $A=\sum_iA_i$ is also Poisson process with rate $\lambda=\sum_i\lambda_i$

{\bf Splitting Property}: Suppose every arrival is randomly routed with probability $p$ to stream 1 and $(1-p)$ to stream 2. Stream 1 and 2 are Poisson with rates $p\lambda$ and $(1-p)\lambda$.

{\bf Kendall's Notation: A/B/C/D} \\
A: inter-arrival time distribution\\
B: service time distribution\\
C: number of servers\\
D: maximum number of jobs possible\\
M:exponential, D: deterministic, G: general\\

$\alpha(t)$: number of arrivals in (0, $t$)\\
$\beta(t)$: number of departures in (0, $t$)\\
$N(t)$: number of customers in system at $t$\\
$T(t)$: average time in system for customer up to $t$\\
$N$: average number of customers in system. $N=N_q+N_s$\\
$T$: average waiting time in system. $T=T_q+T_s$

{\bf Little's Law}: $N=\lambda T$

{\bf Utilization Factor (Traffic Intensity Factor)}: $\rho=\text{Work Arrival Rate } / \text{ Server Capacity}$\\
System is {\bf unstable} if $\rho>1$\\
{\bf Single Server System} \\ $\rho$ is fraction of time server is busy\\
{\bf Multi-Server System} \\ $\rho$ is fraction of busy servers


{\bf M/M/1 Model}: Assume state independent arrival $\lambda$, service rate $\mu$. At equilibrium, flow in $=$ flow out, $\lambda\pi_0=\mu\pi_1\to\pi_1=\frac{\lambda}{\mu}\pi_0$, $\pi_1=\rho\pi_0$. Similarly $\lambda\pi_2=\rho\pi_1=\rho^2\pi_0$. In general $\pi_n=\rho^n\pi_0$. $\sum_i\pi_i=1\to\pi_0=1-\rho$, $\pi_n=(1-\rho)\rho^n$

{\bf Average Number Customers in System $N$}: $\sum_kk\pi_k=\sum_kk(1-\rho)\rho^k=\frac{\rho}{1-\rho}=\frac{\lambda}{\mu-\lambda}$

{\bf Average Time in System $T$}: \\
$T=N/\lambda$ (with Little's Law), $T=\frac{1}{\mu-\lambda}$

{\bf Average Time in Queue $T_q$}: $T_q=T-T_s=\frac{1}{\mu-\lambda}-\frac{1}{\mu}$

{\bf Average Number of Customers in Queue}: $N_q=\lambda T_q=N-\rho$ (with Little's Law)

{\bf Discouraged Arrivals}: Arrival rates decrease with customers in system, $\lambda_k=\frac{\alpha}{k+1}$, $\mu_k=\mu$ Equilibrium state, $p_k=p_0(\frac{\alpha}{\mu})^k\frac{1}{k!}$. Since $\sum_{k=0}^\infty p_k=1$ we find that $p_0=e^{-\alpha/\mu}$. So $p_k=((\frac{\alpha}{\mu})^ke^{-\frac{\alpha}{\mu}})/k!$. {\bf Utilization} $\rho=1-p_0=1-e^{-\frac{\alpha}{\mu}}$

Expected number of customers in system: $N=E[k]=\alpha/\mu$. Expected time in system $T=N/\lambda$. $\lambda_{\text{eff}}$ is expected (effective) arrival rate, $\lambda_{\text{eff}}=\mu\rho=\mu(1-e^{-\alpha/\mu})$, so $T=\alpha/\mu^2(1-e^{-\alpha/\mu})$


{\bf M/M/$\infty$ Model}: Service rate (number of servers) scales linarly with number of customers in system. $\to$ No waiting time, served immediately. $\lambda_k=\lambda$, $\mu_k=k\mu$. Equilibrium state, $p_k=(\frac{\lambda}{\mu})^ke^{-\lambda/\mu}/k!$, $p_0=e^{-\lambda/\mu}$ (same as discouraged arrival system). Expected number of customers $N=E[k]=\lambda/\mu$. Expected time in system $T=N/\lambda=1/\mu$.


{\bf M/M/m Model}: Unlimited waiting room, constant arrival rate $\lambda$, maximum of $m$ servers. Each server has service rate $\mu$. $\lambda_k=\lambda$, $\mu_k=k\mu$ ($k<m$), $m\mu$ ($k\geq m$). Equilibrium distribution, $p_k=\frac{(m\rho)^k}{k!}p_0$ ($k<m$), $\frac{(m\rho)^k}{m!}\frac{1}{m^{k-m}}p_0$ ($k\geq m$) where $\rho=\lambda/m\mu$. When $\rho<1$, $p_0=\Big[\sum_{k=0}^{m-1}\frac{(m\rho)^k}{k!}+\frac{(m\rho)^m}{m!(1-\rho)}\Big]^{-1}$


{\bf M/M/1/K Model}: System has single server and maximum of K customers. Equilibrium state, $p_k=p_0(\frac{\lambda}{\mu})^k$ ($k\leq K$), 0 ($k>K$), where $p_0=\Big(1-\frac{\lambda}{\mu}\Big)/\Big(1-(\frac{\lambda}{\mu})^{K+1}\Big)$


{\bf Finite Population Systems}: World has finite users and each user is either in system or to arrive with exponential time with mean $1/\mu$. $\lambda_k=\lambda(M-k)$ ($0\leq k<M$), 0 (otherwise). $\mu_k=\mu$ ($1\leq k\leq M$), 0 (otherwise). Equilibrium state, $p_k=p_0(\frac{\lambda}{\mu})^k\frac{M!}{(M-k)!}$ ($0\leq k\leq M$), 0 (otherwise). $p_0=\Big[\sum_{k=0}^M(\frac{\lambda}{\mu})^k\frac{M!}{(M-k)!}\Big]^{-1}$



{\bf ==========================}

% QUESTIONS


{\bf Q0}: Using Laplace Transform, solve differential equations $x(t)=\frac{dx(t)}{dt}+2\frac{d^2x(t)}{dt^2}$, $x(0)=1$, $x'(0)=1/2$.

$L\{x'(t)\}=sX(s)-x(0)$\\
$L\{x''(t)=s^2X(x)-sx(0)-x'(0)\}$\\
$X(s)=sX(s)-x(0)+2[s^2X(x)-sx(0)-x'(0)]$\\
$X(s)=1/(s-\frac{1}{2})$, $x(t)=e^{1/2t}$


{\bf Q1}: Program has execution time uniformly distributed between 10 \& 20 seconds. Number of interrupts during execution is Poisson random variable with parameter $\lambda t$ where $t$ is program execution time. Probability distribution of the number of interrupts is $P(N = k) = (\lambda t)ke^{−\lambda t}$.

(a) What is $E[N|T = t]$, where $N$ is the number of interrupts, and $T$ is running time.

$E[N|T = t] = \lambda t$, since for fixed running time, the number of interrupts is a Poisson random variable with mean $\lambda t$.

(b) Expected number interrupts during randomly selected execution.

$E[N] = \int_{10}^{20} E[N|T=t]f_T(t)dt = \int_{10}^{20}\frac{\lambda t}{10}dt = 15\lambda$



{\bf Q2}: We have $m$ types of visitors with equal probability. Find expected visitors to have one of each type. Hint: Let $X$ denote number of visitors needed. Represent $X$ by $X=\sum_{i=1}^mX_i$ where each $X_i$ is a geometric random variable.

Suppose the current visitor pool contains $i$ different types. Let $X_i$ denote the number of additional visitors needed until it contains $i + 1$ types. The $X_i$ is are independent geometric random variables with parameter $(m - i)/m$, $i = 0, 1, \cdots, m-1$. $E[X]=E[\sum_{i=1}^mX_i]=\sum_{i=1}^mE[X_i]=\sum_{i=1}^m\frac{m}{m-i}$



{\bf Q3}: A Markov chain $\{X_n,n\geq0\}$ with states 0, 1, 2 has transition probability matrix $\begin{bmatrix}
1/2 & 1/3 & 1/6 \\
0 & 1/3 & 2/3 \\
1/2 & 0 & 1/2
\end{bmatrix}$
If $P[X_0=0]=P[X_0=1]=\frac{1}{4}$, find state probability vector $P[X_3=2]$.

Cubing transition probability matrix $P^{(3)}=\begin{bmatrix}
13/36 & 11/54 & 47/108 \\
4/9 & 4/27 & 11/27 \\
5/12 & 2/9 & 13/36
\end{bmatrix}$
$P[X_3=2]=\frac{1}{4}\cdot\frac{47}{108}+\frac{1}{4}\cdot\frac{11}{27}+\frac{1}{2}\cdot\frac{13}{36}$



{\bf Q4}: Whether or not collision occurs depends on result of last two transmissions If collisions occurred in both of the past two, collision will occur with probability 0.7; collision occurs in last but not the transmission before the last one, then a collision will occur with probability 0.5; collision occurred in transmission before the last one but not the last one, collision with probability 0.4; no collision in the past two transmissions, collision with probability 0.2.

(b) Find the transition probability matrix.

$P=\begin{bmatrix}0.7 & 0 & 0.3 & 0 \\ 0.5 & 0 & 0.5 & 0 \\ 0 & 0.4 & 0 & 0.6 \\ 0 & 0.2 & 0 & 0.8\end{bmatrix}$

(c) What fraction of frames suffer a collision?

Solve $\pi = \pi P$ to obtain the stationary state probabilities. Then the fraction of frames suffering a collision is $\pi_0 + \pi_2$ (or $\pi_0 + \pi_1$).



{\bf Q5}: A shop has room for 2 customers. Customers arrive at Poisson rate 3/hour and service times are exponential random variables with mean 0.25 hours.

(a) Average number of customers in shop?

We have birth-death process with $\lambda=3$ and $\mu=4$. $E[k] = \sum_{k=0}^2k\pi_k$. Solve for $\pi_k$, $\pi_1=\frac{\lambda}{\mu}\pi_0$, $\pi_2=\frac{\lambda}{\mu}\pi_1$, $\sum_{j=0}^2\pi_j=1$, so $\pi_0=16/37$, $E[k]=30/37$.

(b) What is proportion of customers who get serviced? $\pi_0+\pi_1=28/37$.



{\bf Q6}: Packets arrive at router Poisson rate 3/ms and time to foward exponential with mean 0.2 ms. Fraction of time buffer empty?

State is num packets in router, $\lambda=3$, $\mu=5$. Solve $p_1=\frac{\lambda}{\mu}p_0$, $p_2=\frac{\lambda}{\mu}p_1$, ..., $p_{k+1}=\frac{\lambda}{\mu}p_k$, $\sum_{k=0}^\infty p_k=1$, thus $p_0=2/5$.



{\bf Q7}: 2 machines produce products at $n$ products/hour. Lifetime of machine follows exponential with mean $1/x$ hours, time to fix machine folows exponential with mean $1/y$ hours. Expected long term producing rate?

State is num machines working. $\lambda_0=\lambda_1=y$, $\mu_1=x$, $\mu_2=2x$ bc if either machine fails, num working machines reduce from 2 to 1. Solve $p_1=\frac{y}{x}p_0$, $p_2=\frac{y}{2x}p_1$, $p_0+p_1+p_2=1$, thus $p_0=\frac{2x^2}{2x^2+2xy+y^2}$. Expected num products produced per hour: $p_0\times 0+p_1\times n+p_2\times 2n$



{\bf Q8}: M/M/1 queueing system. Jobs arrive to be scheduled at rate $\lambda$ jobs/ms. Single core serving jobs at rate of $\mu$ jobs/ms. Suppose that the we want the average time spent in waiting queue to be no more than 3 milliseconds. If $\lambda = 10$ jobs/ms, find minimum of $\mu$.

Steady state probabilities are $\pi_0 = 1 − \rho$, $\pi_k =\rho^k(1−\rho)$ where $\rho=\lambda/\mu$. Average customers $N=\frac{\rho}{1-\rho}$. By Little's Law, $N = \lambda\cdot T\to T=N$ where $T$ is average time in system. Average waiting time $W=T-1/\mu$, so we need $W=\frac{\rho}{\lambda(1-\rho)}-\frac{1}{\mu}=\frac{1}{\mu-\lambda}-\frac{1}{\mu}\leq 3$



{\bf Q9}: $M$ customers go to single-server. When customer arrives, enters service if the server is free or joins queue. Upon leaving, customer returns after exponential time with rate $\lambda$. Service time exponentially distributed $\mu$.

(a) Define states and set up the balance equations. States are \# customers in server. Balance equations $M\lambda\pi_0=\mu\pi_1$\\$((M-i)\lambda+\mu)\pi_i=(M0i+1)\lambda\pi_{i-1}+\mu\pi_{i+1}$ for $i=1,2,\cdots,M-1$

(b) Average rate customers enter the station. $\lambda_{\text{avg}}=\sum_{i=0}^M(M-i)\lambda\pi_i$

(c) Average time customer spends. Avg \# customers $N=\sum_{i=0}^Mi\pi_i$. Avg time $T=N/\lambda_{\text{avg}}$



{\bf Q10}: UCLA receives 10 average applications. Each has $0.7$ probability getting accepted. Program takes 2 years. Assume Poisson arrival, exponential time for time length.

What is probability no students in program? What is average number of students in program each year?

Modeled as M/M/$\infty$ system with $\lambda=10\cdot 0.7=7$, $\mu=0.5$. No students in program: $p_0=e^{-\lambda/\mu}$ Average number of students $N=\lambda/\mu=14$.



{\bf Q11}: Two servers. 90 reqs/hour and placed in single buffer one server becomes free. Service time 1.2 minutes. Assuming inter-arrival times and service times exponential. Determine probability  no requests in the system. Expected number of requests in the system. Expected time request waits in buffer.

Modeled as M/M/2 with $\lambda=90$, $\mu=50$. $\rho=\lambda/\mu=1.8$. $\rho/m=1.8/2=0.9<1$. Thus system is stable. $P$[No Requests] $=\Big[\frac{\rho^m}{m!}\frac{m}{m-\rho}+\sum_{n=0}^{m-1}\frac{\rho^n}{n!}\Big]^{-1}$. Expected number of reqs, $N_s=\sum_{n=0}^\infty np_n=\sum_{n=0}^{m-1}np_n+\sum_{n=m}^\infty np_n$, $N_s=\sum_{n=0}^{m-1}n\frac{p^n}{m!}p_0+\sum_{n=m}^\infty \frac{p_n}{m!m^{n-m}p_0}$. Expected time in buffer, $T_q=T_s-1.2(\text{min})$, where $T_s=N_s/\lambda$. $T_q=5.117$ (min).



{\bf Q12}: Computing server has finite buffer with size = 6 requests (excluding the one being handled). Assuming arrival rate of incoming requests is Poisson with average 9 reqs/min, and exponentially distributed service time with average 8 seconds.

What is the average number of request in the the server? What is the average number of requests waiting?



If the fee of one request is 0.25\$ and the server works 12 hours a day, how much money is gained on average each day? How much gain is lost (due to discarded requests) ?



On average, how long does a request have to wait until being handled?



The server administrators decided that they should buy another server, if the old server works more than 85\% of time. should they buy a new one?




\end{multicols*}
\end{document}
