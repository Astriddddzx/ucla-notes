
Computer Science 188 - Advanced Software Construction

http://web.cs.ucla.edu/classes/spring16/cs188-3

Paul Eggert

(1) Case studies of large programs
    - learn by reading code
    - learn by writing/changing code
      * Eggert will supply one example: GNU Emacs
        + Eggert knows it
        + it has been around for some time
        + it has multi-level abstraction
          Elisp extensions (most of Emacs) (modify Emacs by adding modules)
          C core (small but key part)
      * We pick one
        Chrome, GCC, Node.js, ...

(2) One other homework, currently undecided
    probably in micro-services area

20%    GNU Emacs case study
30%    personal case study
10%    other homework
15%    midterm (open book/notes)
25%    final exam (open book/notes)

June 5 drop-dead date, last day of instruction


Eggert lectures schedule
-------------------------
  1  intro
  1  Emacs tour
  1  regular expression
     Perl compatible regular expressions (PCRE)
     case study involves adding PRCR to Emacs
  5  micro-services
  8  continuous delivery & DevOps
  1  midterm
  2  presentations
  1  fun stuff (configuration + startup)


==============
Mar. 29, 2016
==============

--------------------------
By next time read
  Bug #23133
  Cox on RE2 (parts 1-4)
--------------------------

gzip 1.7 (2016-03-28)

--rsyncable: compressing files making gzip work better with program rsync
  rsync : operates on blocks, compares and changes only the changed parts

common in systems with 'deduplication'
file system knows what is going on and does not maintain copies but keeps
track that the copy exists

  $ cp a b
  $ ls -li a b

    247963 -rw-r-r-       193261   a
    257106 -rw-r-r-       193261   b

                   inodes
           +----+
           |   -----------------+        actual data
+---+      +----+    +----+     |    +-----+------+----+
| a -----> |    |    |    |     +--> |     |      |    |
+---+      +----+    +----+     |    +-----+------+----+
| b ---------------> |   -------+
+---+                +----+


file system will figure out that two files are the same and will not keep two
copies in order to save space

(1) since ~1990

(2) gzip.c ought to parse input and invoke zlib to do the real work
    BUT it actually has its own copy
    should we fix this problem?
    if we find a bug in either one, we would have to change both

(3) is gzip the best compression algorithm we have?
  
      0. preserves data
      1. smallest output
      2. fastest compression
      3. fastest decompression
      4. doesn't leak data
      5. uses least RAM compression
      6. uses least RAM decompression

    there isn't a single compression algorithm that dominates here
    gzip came out at right time, was free, and good 'enough'


GNU Bug 23133 (http://bugs.gnu.org/23133)
------------------------------------------
Built gzip on platform Oracle Solaris Studio 12.4 x86-64
has its own C compiler cc

  $ ./configure CC=cc CFLAGS='m64 -g'
  $ make
  $ make check

    ./gzip < gzip.doc > t    ->    dumped core

  $ dbx
  $ (dbx) where

    [1] 0x428885(--gibberish--)  <- nameless function with gibberish arguments
    [2] deflate()
    [3] zip()
    [4] treat_stdin()
    [5] main()

possibilities
  weird shared library
  lambda functions (not directly possible in the C language)


Eggert reproduced the same result on Oracle Solaris Studio 12.3

deflate() was calling longest_match

  static int
  longest_match (IPOS cur_match) {
    // C code here
  }

use gdb to disassemble 0x428885

  pushl (----)
  pushl (----)
  # reasonable instructions #
  # no ret! #

looked at source code and saw guards

  #ifndef ASMV

  static int
  longest_match (IPOS cur_match) {
    // C code here
  }

  #endif

tried to find where this came from
 
  $ grep ASMV *

    config.h #define ASMV

  $ grep -r longest_match /usr/include

  $ grep -r longest_match *

    lib/match.c: longest_match: (no x86-64 version)
    # this was inline assembly inside a C file

      #if defined(i386) || defined(__i386) || ...
        longest_match:
              pushl (....)
              pushl (....)
              ...
              ret
      #endif

here's what ./configure does

  ./configure CC=cc CFLAGS='-m64 -g'
  configure.ac  #Autoconfig input (automatically generates configure scripts)
    if
      cp lib/match.c  _match.S
      cc -E -m64 -g _match.S > _match.S  # do macro expansion
      cc -m64 -c _match.S &&
      test -f _match.o
    then
      AC_DEFINE([ASMV])
    fi

here's the part that took the longest time for Eggert to fix

  cc -m64 -E t.c
    +--------+
    | t.c    |
    +--------+
    | i386   |
    | __i386 |
    +--------+    output looked like input

  cc -m64 -E t.S  <--  preprocessable assembler
    +--------+
    | t.S    |
    +--------+
    | i386   |  -->  1
    | __i386 |  -->  __i386
    +--------+

here's how Eggert fixed the bug
added '&& !defined(__x86_64__)'

  #if (defined(i386) || defined(__i386) || ...) && !defined(__x86_64__)
    longest_match:
          pushl (....)
          pushl (....)
          ...
          ret
  #endif


What can we learn from this bug report and fix?
------------------------------------------------
(1) 'make check' is your friend

(2) asm version was originally important
    maybe remove it now!
    this has become a maintainence problem

(3) gzip should use glib

(4) must view your program at multiple levels down to the machine level code

(5) important to also have a workaround
    ./configure CC='cc -m64' CFLAGS=-g DEFS='-D NO_ASM'
    # this is a hack to skip test for ASMV
    # gcc -D NDEBUG  or  gcc -DNDEBUG

(6) dump Solaris?
         HP-UX
         AIX


this is common, ordinary in "real" software development
--------------------------------------------------------
Q: can this be taught?
A1: yes, but only by doing, and it's not academic
A2: yes, and there is a set of methods that survive technology change


Central problems of software construction
------------------------------------------
this is not software engineering
  one of the biggest problems here is getting requirements right

(1) modularity (mechanism for dividing programs)
    cannot keep program as one big piece, it simply doesn't scale
    have to keep them in manageable pieces
    we don't have a good enought handle yet on modularity

(2) abstraction (good modularity)
    not only break program into pieces but pieces are nicely orgainized
    we can build a layer between low-level hardware and high-level machine
    the line dividing this gives a pattern that is useful in solving problems

(3) flexibility/composability (ease of building/conceiving new programs)
    broken up program into pieces
    other program can substitute pieces into original to build new program
    similar to jigsaw puzzle
    modules that plug together

(4) autonomy
    want different modules to operate on own without worrying about each other
    want each piece to be manageable by a different crew
    able to run each piece without worrying about other piece
    important in cloud-based environment, keeping pieces in different 'clouds'
    even important in monolithic applications, for multi-threading

(5) portability (technology heterogeneity)
    want code to survive changes in technology wihtout changing code too much
    want application to work independent of technology

(6) scalability
    want system to work well regardless of problem size
    want amount of work to grow roughly linearly O(N) with work size
    important for scaling on the cloud
    
    vertical vs. horizonatal scale
    horizontal: just add servers
    vertical: have single server that is powerful

    scaling with code size (solution, not problem here)
    techniques to handle huge programs need to change

(7) ease of replacement
    want to avoid 'too big to fail' problem
    which means there is one huge component that is 'too big' that if it fails
    the whole program has to fail
    depends too much on one single component

(8) approachability/learnability
    can easily learn the code and program structure
    how easy to get new users
    how easy to get developers
    how easy to get operators

(9) ease of deployment (getting improvements to users)
    as we add new funcitonalities, how easy to push out fixes to all users

(10) organizational alignment
     not 'class boundaries map into behavior'
     want to match developer groups to code they write, not object behavior
     java: object hierarchy (behavior), package hierarchy (organization)
     not just ownershp but shifting development resources

(11) developers should not have to wait
     wait to checkin a new version in order to compile
     do not wait for a 'good build'
     should not be waiting in order to start developing
     give developers control over the version they use 


==============
Mar. 31, 2016
==============

Central problems
  modularity
  abstraction
  flexibility
  autonomy
  portability
  scaling
  org alignment
  ease of deployment
  ease of replacement
  developers should not have to wait

(12) ease of configuration
     easy to set things up so that it operates correctly in that environment
     should be automatic if possible
     huge problems lie in configuration errors in practice

(13) risk/stress of new releases
     about to come up with new version and not know what to do
     since new releases are so stressful and risky, do them more often

(14) debugging
     e.g. multithreaded applications (legendarily hard to debug due to race
     conditions)
     
     1. repeating bugs
     2. find the cause of the bug
     3. fixing the bug

(15) performance
     performance improvement is a form of debugging
     because bad performance is a bug


Suppose you have a big software engineering project and it keeps growing,
what can you do?

(1) hire more programmers! (doesn't scale up)
(2) hire better programmers!
(3) improve the software process
(4) improve the code itself


construction techniques you already know
-----------------------------------------
;        sequencing        a = a + 1; b = a + 2;
if       alternations
while    iteration
f(x)     functions & calls (recursion)
         classes (templates, generics) encapsulate common behavior
         protocols (derived from networking)
         introspection ('self-aware' code, program can inspect itself)
         serialization (converting to serial form)
         multithreading (synchronization) break into parallel pieces
         generators | implicit sequences
         iterators  | implicit sequences
         asynchronous funtion calls (callback, signals, asynchronous I/O)
         code libraries (dynamic linking)
         domain specific languages (mutate language to fit problem)
         dependency tracking ('make')
         version control + tracking ('git') (branching, merging, patching)

code is closely related to data
  sequencing is like struct { int x, double y }
  alternations is like union {   }
  iteration is like an array ----------
  recursion is like a tree, stack
  protocols match well to serialization
  introspection such as in Java: class Class (tells the class type)

other technology to cover
--------------------------
(1) macro processing
    #define N 1024
    case study in Emacs has several forms of macroprocessing

(2) memory profiling and debugging
    common problem in applications is memory hog chewing up too much memory
    we profile it and find the cause of such huge memory use
    Emacs has this too

(3) byte-code interpreters
    compile language into high-level byte-code
    design particular virtual machine to run the byte-code
    Emacs has this too

(4) snapshotting/checkpointing + restarting
    doing some computation, want snapshot of current state of computation
    later we can start from the current snapshot
    improve reliability of program
    Emacs has this too

(5) lazy evaluation
    write a program but the interpreter does not execute the program eagerly
    but it puts things off until it es 'forced' to execute (print)
    Haskell uses this approach

(6) junctions (Perl6)
    (a | b | c | d) & (e | f | g)
    system uses lazy evaluation
    we have short-circuit evaluation a || b || c || d

(7) static code analysis
    gcc statically analyzes code
    mundane but also focus of leading technology

(8) metaprogramming & code generating
    Emacs has this too

(9) self-modifying code
    Emacs has this too

(10) automated testing
     Emacs has this too

(11) style checking
     Emacs has this too

what we probably won't cover
-----------------------------
(1) software engineering process
(2) big data, big parallelism (don't have enough resources)


Regular Expressions
--------------------
pattern   a(b|c)*bc
string    dabbcbcbcdabc (4 different matches in same string)

pattern   b(a|b|c|d)*c
string    dabbcbcbcdabc (many more different matches in same string)


software is instructed to find best match instead of all matches
-----------------------------------------------------------------
(1) leftmost
(2) longest

how to implement R.E. search
-----------------------------
from the theory (1960s Thompson)
everything here predates Unix

                  RE      recognizer (NFA: nondeterministic finite automaton)

1 char            a       0 -a-> *1 (dot on state is a match)

concat            XY        -> x0   |      -> y0
                          0 -> x1   | -> 0 -> y1    (combine automatons)
                            -> xn   |      -> yn

alternation       X|Y     0 -> x0
                            -> x1
                            -> xn
                            -> y0
                            -> y1
                            -> yn

closure           X*


covert NFA -> DFA (deterministic finite automaton)
DFA has more states O(2^N)  ->  N: number of NFA states

how to implement a DFA?
------------------------
two dimensional array
rows are labeled by state number 
columns are labeled by input symbols

0 = no match in the array

need runtime representation for accepting state
match table of booleans indexed by state number


interpreter loop:
  int s = 1;
  do {
    c = getchar();
    s = table[s][c];
  } while (s);


table is huge
rows: O(2^N)
cols: |SIGMA| + 1
      number of symbols: ASCII     128
                         UNICODE   2^20 and more


problems in R.E. matching
--------------------------
(1) table can be too big

    use lazy evaluation!
    don't precompute entire table
    just compute a row of this table only when the state is first entered
    entries either is a state number, 0, or -1 (not yet computed)

      if (s < 0)
        s = compute(table+s)

(2) getchar() might be slow

    get from disk may have to wait for disk arm
    use buffering
    take entire string and put into RAM to access direcly
      may run out of RAM (use finite buffer if data is large)

      c = *p++  // p is ptr into string

(3) code doesn't tell us where match was

    if we find a success, we keep track of where that occurred

      match_end = NULL;
      match_start = p;
      ......
      if (match[s])
        match_end = p;

    loop continues but we have found at least one match
    will keep track if we find a longer match
    if match_end is non-NULL, then we have found a match

(4) not addressing leftmost match problem

    a. have a bigger loop that iterates over the string
    b. prepend .* to expression X

         .* X
           ^
           |
           record match_begin here


this is not efficient enough, suppose we do this

  $ grep 'c' file

easiest way in C:
  
  for (p = str; p < lim; p++) {
    if (*p == 'c')
      return 1;
  }
  return 0;

faster method than this:
  
  return !!memchr(p, 'c', lim-p);  // C standard library

    a word at a time (8 byte, say) and EXOR (^) with array of 'c'


=============
Apr. 1, 2016
=============

Goal: add Perl REs to Emacs
Initial hello to get started

(1) searching for numbers
(2) searching for integer ranges

Emacs function (re-search-forward)
specify a RE and will search forward until Emacs find a match

  (re-search-forward "ab*c")

add function 'num-search-forward'

  number: [+-]?[0-9]+ (dependent on base)
  hex digits: [a-f][A-F]

  (num-search-forward 239)

    xyabd239efg
    ^  ->  ^

  string representation is not necessary equal to chars in the source code
  do matching with character matching in string, instead of matching number

may want to do approximate search (number range search)
  
  (num-search-forward 239 [250])

  (num-search-forward NUM &optional NUM2 BASE)
  
  two arguments will allow Emacs to search in range of two numbers, inclusive
  BASE is between 2-16, default is 10
  NUM2 and BASE are both optional
  if there is overflow in text, just ignore

    (num-search-forward 239)

      abcd2399128928379827349822934def0239ghi
                                      ^

talking of overflow
(this a good test case)

  (num-search-forward most-negative-fixnum most-positive-fixnum)
                      -2^62                2^62 - 1

for a range search we can transform the search into the original RE functions

buffers are named strings that we are editing
buffers are big
can change and extend buffers
more heavy weight than strings

visit a file
takes all contents and writes to buffer
when we modify file we are actually modifying buffer

to find the current buffer position use

  (point)

this is where to start searching from
searching will change points



** worth 10% of Emacs grade, 2% of total grade
   due Friday Mar. 8, 2016

should be written in Emacs Lisp
put in file num.el

  (defun num-search-forward ...)

this homework is inspired by 'agrep'

to run the file num.el

  M-x load-file RET
  num.el RET


RE search
----------
DFAs
single-character
fast fixed-string search

  $ grep -F 'ab*c'  # simple string matching

-F specifies that the string is not a RE but a fixed string
this can be implemented faster than RE search

Boyer-Moore string search algorithm
------------------------------------
1. searching for a string of known size
   look at the last character of the pattern first

   performance assumptions
   (1) assume pattern is short, string is long
   (2) matches are rare

   p    abcdef
   s    --------------------------------
             ^
             start from here and see if it is a 'f'

2. when you see a failure, figure out where to jump to next
   failures let you jump ahead

   p    abcdef
   s    --------------------------------
             ^
             suppose it's 'g', then we can jump forward 6 chars
             suppose it's 'b', then we can jump forward 3 chars


what can slow down regular expression searching?
-------------------------------------------------
bytes vs characters

  BCDIC (BCD)
    6-bit code (manufacture specific)
    need translate table to copy from one machine to another
  
  EBCDIC (IBM)
    8-bit code
    wasn't quite standardized

  ASCII (everybody else)
    7-bit code
    extra bit as parady bit for catching transmission errors
    eventually 'won'
    enough for most of English, but not all of it
    missing "" '' with directions
    can't do 'naive' with the i with two dots
    not good enough for foreign languages, e.g. French
    8-bit code (256 chars) would not work

  ISO 8859-1 (8-bit ASCII extension)
    covered many western European languages
  ISO 8859-2
  ISO 8859-5
    cyrillic
  ISO 8859-15
    like -1, except for adding a Euro symbol

  case-folded search
    upper case and lower case dit not match in every encoding
    case folding is encoding dependent
    case folding is also language dependent

unibyte encodings (behavior is language and encoding specific)
  more than 256 chars
  16-bit chars, 65536 chars (code points) (Unicode, 1990s)
  font difference, language difference
  the second issue is that 16-bit chars are not large enough
  -> 32-bit chars

byte-encoding for > 256 chars
multibyte characters, single character but more than 1 8-bit char to represent

(1) Shift-JIS
    ASCII chars as themselves
    if first byte is not ASCII, then next byte is treated as payload,
    so this char is represented as a 2 byte char
    however, second byte might be ASCII char, so when searching, we may not
    have found the first byte but simply treated second byte as ASCII

(2) ISO-2022-JP
    switch character sets
    ESC [JC]
    have a lot of flexibility
    still have same problem as (1) does
    suppose we want to apply binary search to text
    this is not possible for 2022 we have to look at all escape sequences
    through the whole string to determine the character set

(3) UTF-8
    multibyte character has payload of trailing bytes (preceded by 10)
    first byte preceded by 11
    byte with leading 0 is ASCII
    
    +-+----+
    |0|    |    ASCII
    +-+----+

    +--+---+--+---+--+---+
    |11|   |10|   |10|   |  ordinary UTF-8 characters
    +--+---+--+---+--+---+

problems in UTF-8
------------------
(1) can be multiple representations for same character
    strcmp doesn't work, since it works on byte level

    UTF-8 simply disallows it currently
    must use shorter version if it exists

(2) encoding errors
    +--------+
    |10000000|   invalid encoding, not a text file!
    +--------+
       128

** READ ABOUT mbrtowc


=============
Apr. 5, 2016
=============

#include <wchar.h>
size_t mbrtowc (wchar_t *restrict pwc, char const *restrict s,
                size_t n, mbstate_t *restrict ps);

char const *restrict s       start of buffer
              size_t n       number of bytes in buffer
mbstate_t *restrict ps       multibyte conversion state
                             converts to ISO-2-22 & etc
 wchar_t *restrict pwc       result (16/32-bit)

            s|<----------------- n ---------------->|
+------------+-+-+-+-+-+----------------------------+
|            | | | | | |                            |
+------------+-+-+-+-+-+----------------------------+
             |<----->|
               bytes consumed

               returns       total number of bytes consumed
                             
                             1-n  # bytes consumed
                             0    NULL char (interface botch, use strlen?)
                             -2   incomplete char
                                  sequence of bytes that may be valid but
                                  we have run off end of the buffer
                             -1   encoding error (what to do?)
                                  fall over and die
                                  flags can say what to do with encoding errors

                                  In RE matching:
                                  (1) encoding error in string is never
                                      matched with anything, encoding errors
                                      in patterns are syntax errors
                                  (2) they're always an error
                                  (3) treating encoding errors as valid
                                      extend to handle other encodings correct
                                      encoding errors are characters
                                      RE matching can match encoding errors

* restrict: pointer with restrict: whenever code accesses pointer, the pointer
  must be independent of others, that is to say, pointers should not point to
  overlapping memory locations
  important for performance

+----------+-+   +-+---------------+
| 200 char | |   | |    300 char   |
+----------+|+   +|+---------------+
   file1    |     |    file2
            |     |
         encoding error

$ cat file1 file2 > file3

+-----------------+--+---------------------+
|     200 char    |  |       300 char      |
+-----------------+--+---------------------+
                  valid char    

maybe file should be sequences of wchar_t? (MS-Windows)
+ simple encoding, decoding
- more space (secondary storage, memory buffer space)
- incompatible with ASCII, Shif-JIS, EUC

mbrtowc is generic, should work for Shif-JIS or any encoding
Locales: context for behaviors that were originally Unix English,
         but need to be more general

char *setlocale (int category, char const *locale);
e.g. setlocale (LC_CTYPE, "en_US.utf8")
                           language_country.encoding

LC_CTYPE      iswalpha, [[:alpha:]]
LC_COLLATE    strcoll, single letters in Spanish: ll, ch
LC_TIME       date
LC_NUMERIC    strtol_l (...,locale)
LC_MESSAGES   strerror (errno)


locale_t newlocale (int categories, char const *locale, locale_t base);

CATEGORIES set of cateogry values
starts off with BASE locale
change the CATEGORIES in BASE to be the new LOCALE
construct new locale that does that and return it

locale_t uselocale (locale_t l);

set the thread's locale to be 'l'
assumes thread local storage (TLS)

locales are heavy-weight and it would be good for the system to cache commonly
used locales.


different ways to specify regular expressions
----------------------------------------------
(1) globbing
    
    simple, fast, specialized for file names & directories

    char        matches itself         a?b.h
    ?           any 1 char             a?b.h
    [abc]       a or b or c  (1 char)  a[a-z]b.h
    *           zero or more chars     a*b.h

      int glob (char const *restrict pattern, int flags,
                int (*errfunc) (char const *, int),
                glob_t *restrict pglob);

      heavy-weight unbounded work

      pattern    null terminated string, "a?b.h"
      errfunc    error handling function callback
      glob_t     struct {
                   size_t gl_pathc;  //
                   char **gl_pathv;
                 }

(2) POSIX BREs (Basic Regular Expressions)

    most commonly used regular expressions
    used by grep

    BRE patterns      what they match
    ---------------------------------------------------------------------------
    c                 c
    \c                always c evey if c is special (does not work in ranges)
    .                 any char except NULL '\0'
    [abc]             set of single chars (256 bits to represent)
    [^abc]            negated set
    [a-zA-Z]          ranges of characters
    [ab\]             backslash not special
    [ab-]             special if at beginning/end
    [#--?]            starting with 'a' ending with '-'
    [z-a]             * undefined behavior
    [[:alnum:]#?:]    any alpha-numeric char, or any #?:
    [abc^]            '^' is not special now
    [[:alnum:]-#]     * undefined behavior
    [[=a=]]           same equivalence class as 'a' (locale dependent set)
    [[abc]            character set of [abc
    [^]ab]            negated set of ]ab
    XY                concatenations
    \(X\)             X
    X*                0 or more instances
    X\{27\}           exactly 27 instances of X
    X\{27,\}          27 or more instances of X
    X\{27,39\}        27 to 39 instances of X
    ^X                X but only at start of string
    X$                X but only at end of string
    \3                back reference to 3rd parenthesized subexpression
    [[.a.]]           matches simply a

    \([abc][def]*\)[^f]*\1

    precedence: *
                concatenation
                anchors (^$)
    ^ab*c

    a^b  (error)
    a\^b (valid)

    
    can be implemented with NFA-DFA

    matcher implementations
    simplest
      backtracking matcher
    DFA
      must have an escape hatch to match \3
      record earlier matches

      sed s/\(a.*b\)a\(a.*b\)/\1q\2/

      have to know where is the match
      where are the submatches

      longest-leftmost
      pattern XYZ

      +--+-----+-----+-----+
      |  |  X  |  Y  |  Z  |
      +--+-----+-----+-----+

      should we maximize length of X or Y or Z
      which alternate to return?
      (1) X is greedy, find the longest X and apply recursively
          leftmost first
          "obvious"
      (2) Z is generous, find the shortest Z
          strange
          most efficient
          arguable POSFIX

** READ Chapters 1-4 (Newman)


=============
Apr. 7, 2016
=============

microservices
--------------
(1) small
    so have to be focused and cohesive
    doesn't take many programmers to develop and maintain (1-5 programmers)
    purposely avoid the temptation to grow in complexity
    keep it simple to keep focused and cohesive
    can rewrite from scratch in 2 weeks
    don't want it too small
      we would spend time gluing together too many microservices

(2) autonomous
    have virtual CPU and all resources to run by itself
    have separate processes or separate virtual machines
    loosely coupled approach
    usual software engineering practice

(3) services
    communicate via networking protocols
    the communication/connection between microservices are called APIs
    traditionally API is Application Programming Interface
    in C: function signature + behavior
    in microservices: can change to Application Protocol Interface
    API design choice is crucial
    API design takes most of the time

(4) work together
    hard part
    want to break application up into pieces

+ can pick & choose technology
  Java for some part
  C++ for others
  tricky to glue together in one process

+ can migrate 
  migrate C++ to Python environment for scripting style

- overhead: more development hassle
  need more programmmers with knowledge in more than one language
  more "ownership"
  more operational hassle
  more problems when it breaks down
  operation staff has to know all the technology

+ service boundaries are bulkheads
  cannot let one technology breakdown bring down other ones
  they are not perfect
  prevent buffer overrun in one module from directly trashing another's

- networks have more forms of failure
  adds complexity

+ can scale components more independently
  suppose all parts of application live in one part of machine
  memory management is not great and give to one component
  other component may take it away
  in microservices, all components can be controlled

- more of a pain to monitor & manage resources
  have more machines to keep track of

+ deploy more incrementally 
  can deploy pieces one at a time
  hard to do with monolithic version where we can deploy a new version

+ software architecture can match development organization
  can split into teams and give one team one microservice to write
  you have existing teams with existing expertise
  you automate what you have

+ "lack of emotional attachment"
  "egoless programmer"
  for various non-technical reasons, programmers get attached to their code
  microservices can avoid this problem
  if you can throw away 2-week's work of microservices, it won't matter

- deploying incrementally sounds good but it has more compatibility issues
  if new service only adds to the protocol it may be good
  but if we change the service, it may introduce new problems


alternatives to microservices
------------------------------
(1) code libraries
    standard to systems
    formerly the principal way to glue together parts
    (standard component of microservices)
    microservices are built atop libaries, not replace it
    libraries are a standard way to share code, DRY (Don't Repeat Yourself)
    would want to keep using code libraries as much as possible

(2) modules & plugins
    someone has written a big application
    some parts of application is essential to your application
    application specific and/or programming language specific
    Erlang world: reliable
    plugins for browsers
    in general, the coupling tends to be too tight
    connection between modules are too picky, too many constraints imposed
    e.g.  char const * restrict str
    the constraint somestimes is a benefit so you would catch error at compile
    and link time rather than run time


software architects & microservices
------------------------------------
spend lots of time doing design
famous architects design iconic buildings - which are immutable
    Gehry    Disney Hall
most software architects design buildings that are not perfect - mutable
establishing the general framework for the building
most software architects are town planners
town planners have to design for growth since towns grow
most software architects are editors of magazines
articles are not written by editors but by individual authors


architects must say
--------------------
(1) what are the strategic goals?
    e.g. scale to a large number of students (some online)

(2) what are your architectured principles?
    want to reduce intertia
    have consistent interfaces
    don't put all your eggs in one basket
      don't put all instances of students in one SQL instance

(3) what design practices do we prefer?
    prefer REST over HTTP
    continuous deployment
    minimize customization


some benefits from standardization
-----------------------------------
+ interface technology (APIs)
  standardize so different pieces can talk to each other
  not just API technology but also its spirit, the style

+ monitoring and logging
  standardize so everyone can understand

* write prototypes and exemplars
  examples on how to hook stuff together
  service template (description of a service)
  you get to write code [!]

people will want exceptions to your standardization
you can evolve your templates
there will always be exceptions in real-life


build emacs from source code

  $ git clone emacs
  $ cd emacs
  $ ./autogen.sh
  $ ./configure [--enable-gcc-warnings]

      -Wall -Wextra (gcc warning flags)
      -Wunsafe-loop-optimizations       (issue warning if optimization on loop
                                         if assumes integer overflow has
                                         undefined behavior)
      -Wno-unsafe-loop-optimizations

        int i, a, b, c;
        for (i = a; i <= b; c++)
          x[i] = 0;

        --> gcc issues warning because b can be INT_MAX

        memset(x+a, 0, b-a+1)

  $ make


integration issues
-------------------
standard problem: how to change a service model
advice: avoid premature decomposition

API guidelines
---------------
(1) make things easy on the users
    there are many advantages to have a simple API
    it's ok to give up performance
    it's ok to give up some features
    JUST TO KEEP API SIMPLE
    have as few config parameters as possible

(2) hide internal details

(3) changes to the service should not break uses

(4) changes to the service should not add requirements to implementation


integration options
--------------------
(1) shared database

    relational database with particular schema, e.g architect specifies schema
    schema changes can be a big deal
    commited to the relational model
    code replication among services

             A   B   C   D
            ---------------
             |   |   |   |
          +--+---+---+---+--+
          | middleware code |
          +-------+---------+
                  |
          +-------+---------+
          | database server |
          +-------+---------+
                  |
             +----+-----+
             | database |
             +----------+
    
    technical microservice
    this kind of communication is slow since it has to go through whole DB

(2) synchronous vs. asynchronous
    
    synchronous:
    whenever you have a question to ask other part of system, you ask and block
    the caller block until the operation is done

      more traditional and natural on conventional uniprocessor
      sequencing is easier to reason about
      makes the code simpler
      has signals (Ctrl-C) in production anyway
      still have to be prepared for application to be terminated

    asynchronous:
    the caller initiates the operation but does not wait for it to be done
    an operation can be "penidng" while the caller continues

      can give better performance
      users like it when code takes less time
      applications don't hang!

(3) decentralized vs. centralized control
    
    decentralized:
    no central control
    scales better
    removed single point of failure
    choreography
      bunch of dancers dancing around stage
    more autonomy, loose coupling here
    harder to implement and harder too keep track of what's going on

    centralized:
    one coordinator
    doesn't scale as well
    orchestration
      has conductor

(4) remote procedure call (RPC)

    1. technology?
         SOAP
         Java RMI
    
    2. format?
         XML
         JSON
         plain text
         binary

    3. do callers know it's RPC?
         yes, X Windows System
         no, network file system (NFS)

** READ Chapters 5-8 (Newman)


==============
Apr. 12, 2016
==============

personal project
-----------------
(1) survey existing projects & codebase
(2) select one or more
(3) read & understand their cores
----> report (Previous work section of your paper)
      20% project * 30% = 6% of total grade (2 weeks from today)


regular expressions history
----------------------------
(1940s) McCullough & Pitts modeled neural networks as finite state machines
(1940s-1950s) Kleene came up with idea of "regular sets"
Implemented by K. Thompson for 'qed' before Unix (IBM 7090 assembler)
compiled regular expression into machine code
pattern match consisted of only running machine code
to some extent Eggert thinks he 'cheated'

How he did do it?
------------------
- threaded code
    
     ORD.  addq $3, %rax
     ORD.  subl %rax, $rdi
    call.  strlen

  simply make functions for all ordinary operations and call it when needed

    subl %rax, %rdi
    movq (%rbx), %rax
    addq $8, %rbx
    jmp (%rax)

  this will be slower for ordinary instructions since is costs more

    Reverse Polish Notation

    ostrlen (pop string, push length)
    add1 (pops integer, push integer+1)
          +--> use %rsp for the stack pointer 

- indirect threaded code
  
  have extra level of indirection

    strlen(s) + 932
    postrlen --> fn descriptor --> machine code

  advantage is that we can have machine code with arguments
  generate "machine instructions" for a stack machine

  advantage of bytecode
  + can write portable ANSI C code to do this *p++
  + smaller code & files
  + better use of data cache
  + portable to other instances, architectures


Unix 'ed' = regular expressions
'grep' same library as Thompson's library (g/re/p)
--> BREs

other regular expression needs
awk, egrep
--> EREs (harder to implement)

POSIX ERE syntax
-----------------
ERE      matches
c        itself
.        any char except \0, \n (system dependent)
[...]    same as BREs
P*       0 or more instances of P
P+       PP*, one or more instances
P?       0 or 1 instance of P
P{m}     exactly m instances of P
P{m,}    m or more instances of P
P{m,n}   m to n instances of P
P|Q      either P or Q (key implementation of virtual machine)

how to implement P|Q ?
( --> push backtrack point (string location, machine instruction)
P --> try to match P
      if succeeds, jump around Q
      if this fails, will pop and backtrack to try Q
Q --> try to match Q

((a|b)*)*(a|b) --> backtrack points grow

POSIX API
#include <regex.h>
int regcomp regex_t *restrict preg, char const *pattern, int flags);

REG_EXTENDED
REG_ICASE
REG_NEWLINE


int regexc(regex_t const *restrict preg,
           char const *restrict string,
           size_t nmatch, // how many subexpressions interested in
           regmatch_t pmatch[restrict], // array of matches
           int eflags)

rm_so  starting byte offset
rm_eo  ending byte offset

REG_NOTBOL
REG_NOTEOL

used for fine grain control of matches
-1 if not participate

Emacs EREs
-----------
X*?
X+?   ==>  non-greedy match (shortest left-most)
X??

X\|Y
\(X\)

\(?:X\) ==> shy group (cannot be backrefered to)

\`   only match beginning of string, match empty string in context
\'   only match end of string, match empty string in context
\=   matches point
\b   word boundary
\B   not a word boundary
\<   beginning of word
\>   end of word
\_<  beginning of symbol
\_>  end of symbol

\sc  syntax is C
\Sc  syntax is not C


Emacs thinks of '\0' == 0 as just being the char NUL

Emacs buffer

+-----------------+-------------+---------------+
| start of buffer |     gap     | end of buffer |
+-----------------+-------------+---------------+
                         |
                         +---> + buffer contents


==============
Apr. 14, 2016
==============

core part of Emacs interpreter
-------------------------------

(1) central part is simple!
(2) based on pointer tagging
    Lisp: runtime type checking
    C: static type checking

    use a union
           +------------+--------------+
    ptr -> | type field |              |
           +------------+--------------+

    union {
      struct type1 {int field; int val; } i;
      struct type2 {int field; double val; } d;
      struct type3 {int field; char buf[100]; } b;
    } u;

    if (u.i.field == 0) // int type
    if (u.d.field == 1) // double type
    if (u.b.field == 2) // buffer type

    or we can use

    struct {
      int field;
      union { int ival; double dval; }
    } u;

    +------+------+
    | cons | flag |   bottom 3 bits flag indicates the type
    +------+------+
           |        +--------+--------+
           +------> |        |        |
                    +--------+--------+

    now we can simply check the flag for type information here


Lisp_Object type = EMACS_INT, normally
  -DCHECK_LISP_OBJECT_TYPE
wrong code: printf("%d", o); // Lisp_Object o
  further checks your code, why not always do this?
  bad performance, ABI conventions
  gcc has a flag to bypass this and generate efficient code

#undef CHECK_LISP_OBJECT_TYPE
enum CHECK_LISP_OBJECT_TYPE { CHECK_LISP_OBJECT_TYPE = true };
  just to not use macros

sometimes we use the top 3 bits rather than the bottom 3 bits for flag
helpful for 32-bit machines where objects can't be forced to 8-byte alignment
limits the memory that can be allocated

NaN tagging
  Lisp_Object = double (native type for numbers)
  use floating point to represent runtime object

  +---+-------+---------------------+
  | s |   e   |          f          |
  +---+-------+---------------------+
       
       11111111 = inf, f = 0
                  NaN, (pointer)

# define lisp_h_XTYPE(a) ((enum Lisp_Type) (XLI (a) & ~VALMASK))
# define lisp_h_XUNTAG(a, type) ((void *) (XLI (a) - (type)))

XLI(l) = i
XIL(i) = l <- Lisp_Object
usage of this should be rare

XCONS(o) -> cdr
((struct Lisp_Cons *) (o - 2)) -> cdr

struct Lisp_Cons {
  Lisp_Object car;
  Lisp_Object cdr;
}

# define lisp_h_XINT(a) (XLI (a) >> INTTYPEBITS) // shifts right by 2 bytes
# define lisp_h_XFASTINT(a) XINT (a)

most-positive-fixnum 2^61 - 1


#include <stdint.h>
#include <inttypes.h>

intptr_t
uintptr_t

these two are wide enough to hold pointers
optional
nearly ubiquitous

INTPTR_MAX
INTPTR_MIN
UINTPTR_MAX

-DWIDE_EMACS_INT
  then we are going to make EMACS_INT EMACS_UINT 64 bits
  support large integers on 32-bit machines

# define pI "l" // used in printf()

* Use GNU Multiple Precision to make integers unlimited

void *p;
intptr_t = (intptr_t)p;
void *q = (void *)p;    // we can get the original pointer back


some machines have 256-bit pointers for security: more information about memory


GCALIGNMENT = 8
struct foo alignas(8) { } s;

needs support from compiler
                   asm
                   linker
                   loader
for C11


-DENABLE_CHECKING
enables extra assertion

# define eassert(cond) ((void) (false && (cond))) // check COND compiles
# define eassert(cond)
  (supporess_checking || (cond)
  ? (void) 0
  : die (# cond, __FILE__, __LINE__))

# define eassume(cond)
   (supress_checking
   ? assume (cond)
   : (cond)
   ? (void) 0
   : die (# cond, __FILE__, __LINE__))


#define EXTERNALLY_VISIBLE __attribute__((externally_visible))
to make property visible to other modules or globally (debugger)
don't optimize away unused variables

eassume(i < n)
i < i + 1; // can be assumed to be true and can turn into true


LISP_MACRO_DEFUN (XCONS, struct Lisp_Cons *, (Lisp_Object a), (a))

a = 10006
return 10000

#define LISP_MACRO_DEFUN(name, type, argdecls, args)
  INLINE type (name) argdecls { return lisp_h_##name args; }

INLINE struct Lisp_Cons *
(XCONS) (Lisp_Object a)
{
  return lisp_h_XCONS (a);
}

define an inline function same as macro XCONS

#define lisp_h_XCONS(a)
  (eassert (CONSP (a)), (struct Lisp_Cons *) XUNTAG (a, Lisp_Cons))

using both functions & macros is for debugging
we can call functions as part of program in debugger
however we cannot call macros in the debugger

  $ (gdb) p malloc(27) # allocate 27 byte and print pointer position
  $ (gdb) p XCONS(o)


INLINE struct Lisp_Vector *
XVECTOR (Lisp_Object a)
{
  eassert(VECTORLIKEP (a));
  return XUNTAG (a, Lisp_Vectorlike);
}


INLINE Lisp_Object
make_lisp_ptr (void *ptr, enum Lisp_Type type)
{
  EMACS_UINT utype = type;
  EMACS_UINT typebits = USE_LSB_TAG ? type : utype << VALBITS;
  Lisp_Object a = XIL (typebits | (uintptr_t) ptr);
  eassert
}


struct Lisp_Vector
{
  struct vectorlike_header header;
  Lisp_Object contents[FLEXIBLE_ARRAY_MEMBER];
}


struct vectorlike_header
{
  ptrdiff_t size;
};

<stddef.h>
size_t     sizeof(foo)

ptrdiff_t
holds difference between two pointers
goes up to 2^63 - 1

In C, unsigned arithmetic does not work as it intends to work

unsigned u = 0;
if (u < -1)
  ouch();

gcc -fsanitize=undefined


==============
Apr. 19, 2016
==============

how functions are defined in Emacs
-----------------------------------

Lisp_Subr

+---------------------+
|    vector header    |
+---------------------+
|                    *----> C function that implements
+----------+----------+
| min_args | max_args |
+----------+----------+
|     symbol_name     |
+---------------------+
|   interactive spec  |
+---------------------+
|         doc         |
+---------------------+

(foo 3 4) => (foo 3 4 nil nil nil) optional arguments are filled in with nil

would be nice to have generic 'apply' function  e.g. apply(f, 5, 0)


static struct Lisp_Subr alignas(8)
scons = {
  (big #),
  .a2 = "Fcons",
  2, 2, "cons"
  "cons", 0, 0 };

+---------------------+
|    vector header    |
+---------------------+
|                    *----> Fcons function in machine code
+----------+----------+
|     2    |     2    |
+----------+----------+
|                    *----> "cons"
+---------------------+
|          0          |
+---------------------+
|          0          |
+---------------------+

DEFUN ("cons", Fcons, Scons, 2, 2, 0,
       doc: /* Create a new cons, give it CAR and CDR as component, and return
       it. */)

interpreter eventually translates the comments into strings

#define DEFSYM(sym, name)
  do { (sym) = intern_c_string ((name)); staticpro (&(sym)); } while (false)

(defun foo (a b &optional c d e)
  (or (< i 5)
      (> i 10)))

'or' does short circuiting


DEFSYM(Qnil, "nil")
       (At, "t")

{
  union {
      Lisp_Object valuel;
      struct Lisp_Symbol *alias;
      struct Lisp_Buffer_Local_Value *blv;
      union Lisp_Fwd *fwd;
    } val;
  
    Lisp_Object function;
    Lisp_Object plist;
    struct Lisp_Symbol *next;
}


integration options
  
  shared database
  sync vs async
  choreography vs orchestration
  RPC

Representational State Transfer (REST)
---------------------------------------
(1) "stateless"
    server doesn't track the client's state
    trying to decouple the client from the server
    one less thing to go wrong, engineer

(2) layered
    client doens't care whether the server is primary

        client
           |
        chache
           |
        server

(3) chacheable
    client able to run independently of the server
    responses from the server state whether they're cacheable

(4) code on demand (optional)
    want the client to be easily changeable, programmable
    JavaScript common

standardish interfacce
want to be flexible
URIs [pointer] in HTML/XML/JSON (small set) [data]
self-descriptive messages
Hypertext As The Engine Of Application State (HATEOAS)

  <album>
    <name>Bachianas Briasteria</name>
    <link rel='/artist' href='/volunteer'>
  </album>

server tells client what can happen next, via URIs in response
efficiency is secondary

(5) less automation than traditional RPC
    RPC has more and better tools

(6) some HTTP verbs not well supporteed
    GET supported

(7) performance (vs RPC)
    latency
    multiple requests

(8) DRY (Don't Repeat Yourself)
    have only one piece of code to do same function
    add a user, change a password
    for code & for data

    diverging copies
    versioning (change API)
    - avoid it when possible (Newman)
      changing the API is painful for all users and clients
      + use good APIs
      + use flexible API, can be easily extended without breaking existing uses

          Old Emacs:   (current-time-string)
      Current Emacs:   (current-time-string &optional zone)

    - semantic versioning
      major.minor.patenleve (3.5.2)
      API that breaks old use
      API but only extension

    can be part of protocol
    server can support multiple APIs at the same time
    like HTTP: 1.0 -> 1.1 -> 2.0

Postel's Law
  accept liberally
  generate conservatively



































